{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nirvanK11/CartPole/blob/main/CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVChq2nOySmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80eb0102-bff6-4fe5-9643-4f12f5120297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxwiby6ROc_C",
        "outputId": "cc187456-5f15-4a3c-e5e2-675d1cfacccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "metadata": {
        "id": "rife0c3dOc8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory \n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "memory = SequentialMemory(limit=500000, window_length=1)\n",
        "\n",
        "policy =  LinearAnnealedPolicy(inner_policy=EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=1.0,\n",
        "                               value_min=0.1, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=10000)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "# add extra layers here\n",
        "model.add(Dense(16, activation='relu'))\n",
        "#model.add(Dense(8, activation='relu'))\n",
        "#model.add(Dense(8, activation='relu'))\n",
        "\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=10,\n",
        "               target_model_update=1e-3, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jvP5meaD2PUc",
        "outputId": "aa0fdf82-d17f-413a-8b05-3fb9d86b35f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_11 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   14/10000: episode: 1, duration: 4.732s, episode steps:  14, steps per second:   3, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.808252, mae: 0.787118, mean_q: -0.092862, mean_eps: 0.998920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   31/10000: episode: 2, duration: 0.340s, episode steps:  17, steps per second:  50, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.690002, mae: 0.687842, mean_q: -0.009027, mean_eps: 0.998020\n",
            "   51/10000: episode: 3, duration: 0.467s, episode steps:  20, steps per second:  43, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.518049, mae: 0.575695, mean_q: 0.135723, mean_eps: 0.996355\n",
            "   65/10000: episode: 4, duration: 0.285s, episode steps:  14, steps per second:  49, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.423167, mae: 0.558288, mean_q: 0.249271, mean_eps: 0.994825\n",
            "   86/10000: episode: 5, duration: 0.538s, episode steps:  21, steps per second:  39, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 0.397646, mae: 0.550654, mean_q: 0.275922, mean_eps: 0.993250\n",
            "  100/10000: episode: 6, duration: 0.297s, episode steps:  14, steps per second:  47, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.342693, mae: 0.525235, mean_q: 0.304568, mean_eps: 0.991675\n",
            "  111/10000: episode: 7, duration: 0.237s, episode steps:  11, steps per second:  46, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.289286, mae: 0.500244, mean_q: 0.355956, mean_eps: 0.990550\n",
            "  146/10000: episode: 8, duration: 0.765s, episode steps:  35, steps per second:  46, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 0.245525, mae: 0.505960, mean_q: 0.472201, mean_eps: 0.988480\n",
            "  157/10000: episode: 9, duration: 0.242s, episode steps:  11, steps per second:  45, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.194493, mae: 0.505576, mean_q: 0.569835, mean_eps: 0.986410\n",
            "  193/10000: episode: 10, duration: 0.674s, episode steps:  36, steps per second:  53, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.147399, mae: 0.518240, mean_q: 0.642684, mean_eps: 0.984295\n",
            "  204/10000: episode: 11, duration: 0.188s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.102335, mae: 0.530140, mean_q: 0.724800, mean_eps: 0.982180\n",
            "  216/10000: episode: 12, duration: 0.228s, episode steps:  12, steps per second:  53, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.088162, mae: 0.537838, mean_q: 0.762209, mean_eps: 0.981145\n",
            "  230/10000: episode: 13, duration: 0.234s, episode steps:  14, steps per second:  60, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.067173, mae: 0.538679, mean_q: 0.822128, mean_eps: 0.979975\n",
            "  248/10000: episode: 14, duration: 0.275s, episode steps:  18, steps per second:  65, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.052760, mae: 0.554915, mean_q: 0.885387, mean_eps: 0.978535\n",
            "  263/10000: episode: 15, duration: 0.240s, episode steps:  15, steps per second:  63, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.041799, mae: 0.550644, mean_q: 0.892919, mean_eps: 0.977050\n",
            "  281/10000: episode: 16, duration: 0.335s, episode steps:  18, steps per second:  54, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.034069, mae: 0.573155, mean_q: 0.968570, mean_eps: 0.975565\n",
            "  299/10000: episode: 17, duration: 0.288s, episode steps:  18, steps per second:  63, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.027019, mae: 0.571595, mean_q: 0.993719, mean_eps: 0.973945\n",
            "  311/10000: episode: 18, duration: 0.237s, episode steps:  12, steps per second:  51, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.023284, mae: 0.584672, mean_q: 1.039391, mean_eps: 0.972595\n",
            "  336/10000: episode: 19, duration: 0.566s, episode steps:  25, steps per second:  44, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.021435, mae: 0.593853, mean_q: 1.071610, mean_eps: 0.970930\n",
            "  348/10000: episode: 20, duration: 0.265s, episode steps:  12, steps per second:  45, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.016994, mae: 0.596156, mean_q: 1.101429, mean_eps: 0.969265\n",
            "  381/10000: episode: 21, duration: 0.604s, episode steps:  33, steps per second:  55, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.016912, mae: 0.613365, mean_q: 1.144676, mean_eps: 0.967240\n",
            "  400/10000: episode: 22, duration: 0.358s, episode steps:  19, steps per second:  53, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.014561, mae: 0.617751, mean_q: 1.157098, mean_eps: 0.964900\n",
            "  419/10000: episode: 23, duration: 0.411s, episode steps:  19, steps per second:  46, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.013039, mae: 0.623388, mean_q: 1.176579, mean_eps: 0.963190\n",
            "  439/10000: episode: 24, duration: 0.400s, episode steps:  20, steps per second:  50, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 0.009574, mae: 0.621797, mean_q: 1.194964, mean_eps: 0.961435\n",
            "  451/10000: episode: 25, duration: 0.224s, episode steps:  12, steps per second:  53, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.013220, mae: 0.641737, mean_q: 1.233593, mean_eps: 0.959995\n",
            "  462/10000: episode: 26, duration: 0.230s, episode steps:  11, steps per second:  48, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.013181, mae: 0.650643, mean_q: 1.254460, mean_eps: 0.958960\n",
            "  478/10000: episode: 27, duration: 0.335s, episode steps:  16, steps per second:  48, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.011994, mae: 0.652959, mean_q: 1.255272, mean_eps: 0.957745\n",
            "  512/10000: episode: 28, duration: 0.576s, episode steps:  34, steps per second:  59, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  loss: 0.008042, mae: 0.647317, mean_q: 1.269504, mean_eps: 0.955495\n",
            "  533/10000: episode: 29, duration: 0.233s, episode steps:  21, steps per second:  90, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.010083, mae: 0.666634, mean_q: 1.308267, mean_eps: 0.953020\n",
            "  552/10000: episode: 30, duration: 0.167s, episode steps:  19, steps per second: 114, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.009469, mae: 0.670103, mean_q: 1.325033, mean_eps: 0.951220\n",
            "  566/10000: episode: 31, duration: 0.129s, episode steps:  14, steps per second: 108, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.008484, mae: 0.675483, mean_q: 1.348461, mean_eps: 0.949735\n",
            "  585/10000: episode: 32, duration: 0.183s, episode steps:  19, steps per second: 104, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.008304, mae: 0.680344, mean_q: 1.364567, mean_eps: 0.948250\n",
            "  603/10000: episode: 33, duration: 0.167s, episode steps:  18, steps per second: 108, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.008489, mae: 0.684968, mean_q: 1.366737, mean_eps: 0.946585\n",
            "  614/10000: episode: 34, duration: 0.108s, episode steps:  11, steps per second: 102, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.011109, mae: 0.701157, mean_q: 1.398943, mean_eps: 0.945280\n",
            "  628/10000: episode: 35, duration: 0.133s, episode steps:  14, steps per second: 106, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.012939, mae: 0.712035, mean_q: 1.397627, mean_eps: 0.944155\n",
            "  639/10000: episode: 36, duration: 0.099s, episode steps:  11, steps per second: 112, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.010187, mae: 0.704761, mean_q: 1.394633, mean_eps: 0.943030\n",
            "  653/10000: episode: 37, duration: 0.129s, episode steps:  14, steps per second: 108, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.012039, mae: 0.720460, mean_q: 1.425915, mean_eps: 0.941905\n",
            "  668/10000: episode: 38, duration: 0.143s, episode steps:  15, steps per second: 105, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.012211, mae: 0.721157, mean_q: 1.430991, mean_eps: 0.940600\n",
            "  684/10000: episode: 39, duration: 0.159s, episode steps:  16, steps per second: 101, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.010817, mae: 0.724654, mean_q: 1.440359, mean_eps: 0.939205\n",
            "  771/10000: episode: 40, duration: 0.751s, episode steps:  87, steps per second: 116, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.563 [0.000, 1.000],  loss: 0.014032, mae: 0.744754, mean_q: 1.466534, mean_eps: 0.934570\n",
            "  801/10000: episode: 41, duration: 0.272s, episode steps:  30, steps per second: 110, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.011747, mae: 0.759301, mean_q: 1.531289, mean_eps: 0.929305\n",
            "  849/10000: episode: 42, duration: 0.442s, episode steps:  48, steps per second: 109, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.016064, mae: 0.777553, mean_q: 1.546886, mean_eps: 0.925795\n",
            "  878/10000: episode: 43, duration: 0.259s, episode steps:  29, steps per second: 112, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.621 [0.000, 1.000],  loss: 0.017374, mae: 0.793972, mean_q: 1.581766, mean_eps: 0.922330\n",
            "  900/10000: episode: 44, duration: 0.189s, episode steps:  22, steps per second: 116, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.012002, mae: 0.793176, mean_q: 1.594633, mean_eps: 0.920035\n",
            "  914/10000: episode: 45, duration: 0.143s, episode steps:  14, steps per second:  98, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.014353, mae: 0.805701, mean_q: 1.616774, mean_eps: 0.918415\n",
            "  933/10000: episode: 46, duration: 0.171s, episode steps:  19, steps per second: 111, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.013627, mae: 0.811248, mean_q: 1.627436, mean_eps: 0.916930\n",
            "  948/10000: episode: 47, duration: 0.146s, episode steps:  15, steps per second: 102, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.009715, mae: 0.820480, mean_q: 1.663568, mean_eps: 0.915400\n",
            "  995/10000: episode: 48, duration: 0.421s, episode steps:  47, steps per second: 112, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 0.013201, mae: 0.834913, mean_q: 1.678674, mean_eps: 0.912610\n",
            " 1007/10000: episode: 49, duration: 0.113s, episode steps:  12, steps per second: 106, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.012545, mae: 0.849332, mean_q: 1.701006, mean_eps: 0.909955\n",
            " 1045/10000: episode: 50, duration: 0.350s, episode steps:  38, steps per second: 108, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.018130, mae: 0.857966, mean_q: 1.715289, mean_eps: 0.907705\n",
            " 1066/10000: episode: 51, duration: 0.180s, episode steps:  21, steps per second: 117, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.016702, mae: 0.870324, mean_q: 1.741038, mean_eps: 0.905050\n",
            " 1080/10000: episode: 52, duration: 0.128s, episode steps:  14, steps per second: 109, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.018200, mae: 0.885704, mean_q: 1.793347, mean_eps: 0.903475\n",
            " 1093/10000: episode: 53, duration: 0.120s, episode steps:  13, steps per second: 108, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.016763, mae: 0.880362, mean_q: 1.787455, mean_eps: 0.902260\n",
            " 1112/10000: episode: 54, duration: 0.176s, episode steps:  19, steps per second: 108, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.015250, mae: 0.879428, mean_q: 1.775767, mean_eps: 0.900820\n",
            " 1138/10000: episode: 55, duration: 0.247s, episode steps:  26, steps per second: 105, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.015468, mae: 0.884952, mean_q: 1.779834, mean_eps: 0.898795\n",
            " 1155/10000: episode: 56, duration: 0.152s, episode steps:  17, steps per second: 112, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.015704, mae: 0.903420, mean_q: 1.814631, mean_eps: 0.896860\n",
            " 1173/10000: episode: 57, duration: 0.168s, episode steps:  18, steps per second: 107, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.019153, mae: 0.901814, mean_q: 1.820425, mean_eps: 0.895285\n",
            " 1184/10000: episode: 58, duration: 0.113s, episode steps:  11, steps per second:  98, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.013289, mae: 0.901692, mean_q: 1.844715, mean_eps: 0.893980\n",
            " 1214/10000: episode: 59, duration: 0.277s, episode steps:  30, steps per second: 108, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.020837, mae: 0.927268, mean_q: 1.839265, mean_eps: 0.892135\n",
            " 1238/10000: episode: 60, duration: 0.236s, episode steps:  24, steps per second: 102, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.014076, mae: 0.926792, mean_q: 1.861249, mean_eps: 0.889705\n",
            " 1252/10000: episode: 61, duration: 0.130s, episode steps:  14, steps per second: 108, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.019449, mae: 0.934036, mean_q: 1.886421, mean_eps: 0.887995\n",
            " 1265/10000: episode: 62, duration: 0.132s, episode steps:  13, steps per second:  99, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.023816, mae: 0.938660, mean_q: 1.867836, mean_eps: 0.886780\n",
            " 1282/10000: episode: 63, duration: 0.154s, episode steps:  17, steps per second: 111, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.019347, mae: 0.944839, mean_q: 1.886026, mean_eps: 0.885430\n",
            " 1296/10000: episode: 64, duration: 0.138s, episode steps:  14, steps per second: 102, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.020065, mae: 0.953588, mean_q: 1.905294, mean_eps: 0.884035\n",
            " 1315/10000: episode: 65, duration: 0.188s, episode steps:  19, steps per second: 101, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.024192, mae: 0.968617, mean_q: 1.924236, mean_eps: 0.882550\n",
            " 1354/10000: episode: 66, duration: 0.354s, episode steps:  39, steps per second: 110, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  loss: 0.021735, mae: 0.975807, mean_q: 1.961325, mean_eps: 0.879940\n",
            " 1382/10000: episode: 67, duration: 0.244s, episode steps:  28, steps per second: 115, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.021622, mae: 0.993823, mean_q: 1.980181, mean_eps: 0.876925\n",
            " 1407/10000: episode: 68, duration: 0.222s, episode steps:  25, steps per second: 113, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 0.023656, mae: 1.006677, mean_q: 2.020137, mean_eps: 0.874540\n",
            " 1434/10000: episode: 69, duration: 0.244s, episode steps:  27, steps per second: 111, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.023633, mae: 1.013938, mean_q: 2.031211, mean_eps: 0.872200\n",
            " 1449/10000: episode: 70, duration: 0.135s, episode steps:  15, steps per second: 111, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.028427, mae: 1.022226, mean_q: 2.045817, mean_eps: 0.870310\n",
            " 1461/10000: episode: 71, duration: 0.127s, episode steps:  12, steps per second:  95, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.018199, mae: 1.017820, mean_q: 2.062422, mean_eps: 0.869095\n",
            " 1485/10000: episode: 72, duration: 0.216s, episode steps:  24, steps per second: 111, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.022846, mae: 1.039085, mean_q: 2.084901, mean_eps: 0.867475\n",
            " 1498/10000: episode: 73, duration: 0.114s, episode steps:  13, steps per second: 114, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.028870, mae: 1.039822, mean_q: 2.094291, mean_eps: 0.865810\n",
            " 1511/10000: episode: 74, duration: 0.124s, episode steps:  13, steps per second: 105, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.030088, mae: 1.043324, mean_q: 2.079019, mean_eps: 0.864640\n",
            " 1527/10000: episode: 75, duration: 0.146s, episode steps:  16, steps per second: 110, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.028135, mae: 1.057665, mean_q: 2.115957, mean_eps: 0.863335\n",
            " 1539/10000: episode: 76, duration: 0.111s, episode steps:  12, steps per second: 108, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.020451, mae: 1.057339, mean_q: 2.146701, mean_eps: 0.862075\n",
            " 1555/10000: episode: 77, duration: 0.148s, episode steps:  16, steps per second: 108, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.029847, mae: 1.070553, mean_q: 2.146043, mean_eps: 0.860815\n",
            " 1577/10000: episode: 78, duration: 0.214s, episode steps:  22, steps per second: 103, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.019654, mae: 1.072634, mean_q: 2.162752, mean_eps: 0.859105\n",
            " 1591/10000: episode: 79, duration: 0.130s, episode steps:  14, steps per second: 108, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.030625, mae: 1.082810, mean_q: 2.166602, mean_eps: 0.857485\n",
            " 1615/10000: episode: 80, duration: 0.211s, episode steps:  24, steps per second: 114, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.028222, mae: 1.095018, mean_q: 2.186082, mean_eps: 0.855775\n",
            " 1648/10000: episode: 81, duration: 0.293s, episode steps:  33, steps per second: 113, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.027361, mae: 1.097840, mean_q: 2.191182, mean_eps: 0.853210\n",
            " 1664/10000: episode: 82, duration: 0.149s, episode steps:  16, steps per second: 107, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.025040, mae: 1.110949, mean_q: 2.248938, mean_eps: 0.851005\n",
            " 1684/10000: episode: 83, duration: 0.197s, episode steps:  20, steps per second: 102, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.020949, mae: 1.119640, mean_q: 2.263231, mean_eps: 0.849385\n",
            " 1703/10000: episode: 84, duration: 0.178s, episode steps:  19, steps per second: 107, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.033534, mae: 1.141360, mean_q: 2.282351, mean_eps: 0.847630\n",
            " 1714/10000: episode: 85, duration: 0.104s, episode steps:  11, steps per second: 106, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.023525, mae: 1.125688, mean_q: 2.242231, mean_eps: 0.846280\n",
            " 1742/10000: episode: 86, duration: 0.272s, episode steps:  28, steps per second: 103, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.033848, mae: 1.148655, mean_q: 2.285499, mean_eps: 0.844525\n",
            " 1753/10000: episode: 87, duration: 0.120s, episode steps:  11, steps per second:  92, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.027788, mae: 1.145930, mean_q: 2.316769, mean_eps: 0.842770\n",
            " 1773/10000: episode: 88, duration: 0.196s, episode steps:  20, steps per second: 102, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.031030, mae: 1.162086, mean_q: 2.347072, mean_eps: 0.841375\n",
            " 1789/10000: episode: 89, duration: 0.163s, episode steps:  16, steps per second:  98, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.041614, mae: 1.169393, mean_q: 2.333294, mean_eps: 0.839755\n",
            " 1807/10000: episode: 90, duration: 0.170s, episode steps:  18, steps per second: 106, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.039359, mae: 1.191229, mean_q: 2.372592, mean_eps: 0.838225\n",
            " 1819/10000: episode: 91, duration: 0.119s, episode steps:  12, steps per second: 100, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.043008, mae: 1.186267, mean_q: 2.378121, mean_eps: 0.836875\n",
            " 1837/10000: episode: 92, duration: 0.161s, episode steps:  18, steps per second: 112, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.046952, mae: 1.199340, mean_q: 2.346954, mean_eps: 0.835525\n",
            " 1849/10000: episode: 93, duration: 0.108s, episode steps:  12, steps per second: 111, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.018835, mae: 1.191593, mean_q: 2.391722, mean_eps: 0.834175\n",
            " 1859/10000: episode: 94, duration: 0.090s, episode steps:  10, steps per second: 111, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.055739, mae: 1.228065, mean_q: 2.419357, mean_eps: 0.833185\n",
            " 1959/10000: episode: 95, duration: 0.899s, episode steps: 100, steps per second: 111, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 0.040766, mae: 1.231970, mean_q: 2.454078, mean_eps: 0.828235\n",
            " 1983/10000: episode: 96, duration: 0.225s, episode steps:  24, steps per second: 107, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.038415, mae: 1.264357, mean_q: 2.499792, mean_eps: 0.822655\n",
            " 1996/10000: episode: 97, duration: 0.121s, episode steps:  13, steps per second: 107, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.035333, mae: 1.277719, mean_q: 2.550777, mean_eps: 0.820990\n",
            " 2007/10000: episode: 98, duration: 0.128s, episode steps:  11, steps per second:  86, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.043861, mae: 1.265140, mean_q: 2.535891, mean_eps: 0.819910\n",
            " 2021/10000: episode: 99, duration: 0.132s, episode steps:  14, steps per second: 106, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.052935, mae: 1.270515, mean_q: 2.533778, mean_eps: 0.818785\n",
            " 2038/10000: episode: 100, duration: 0.163s, episode steps:  17, steps per second: 105, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.040513, mae: 1.280512, mean_q: 2.551155, mean_eps: 0.817390\n",
            " 2048/10000: episode: 101, duration: 0.104s, episode steps:  10, steps per second:  96, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.038172, mae: 1.273883, mean_q: 2.533155, mean_eps: 0.816175\n",
            " 2077/10000: episode: 102, duration: 0.271s, episode steps:  29, steps per second: 107, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.621 [0.000, 1.000],  loss: 0.043966, mae: 1.315624, mean_q: 2.596877, mean_eps: 0.814420\n",
            " 2093/10000: episode: 103, duration: 0.154s, episode steps:  16, steps per second: 104, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.040973, mae: 1.307830, mean_q: 2.574341, mean_eps: 0.812395\n",
            " 2119/10000: episode: 104, duration: 0.261s, episode steps:  26, steps per second:  99, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.654 [0.000, 1.000],  loss: 0.043076, mae: 1.317405, mean_q: 2.634022, mean_eps: 0.810505\n",
            " 2132/10000: episode: 105, duration: 0.126s, episode steps:  13, steps per second: 103, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.059410, mae: 1.345302, mean_q: 2.639436, mean_eps: 0.808750\n",
            " 2154/10000: episode: 106, duration: 0.207s, episode steps:  22, steps per second: 107, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.048247, mae: 1.337616, mean_q: 2.619205, mean_eps: 0.807175\n",
            " 2175/10000: episode: 107, duration: 0.183s, episode steps:  21, steps per second: 115, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.050142, mae: 1.355125, mean_q: 2.683251, mean_eps: 0.805240\n",
            " 2190/10000: episode: 108, duration: 0.142s, episode steps:  15, steps per second: 106, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.055452, mae: 1.358164, mean_q: 2.687912, mean_eps: 0.803620\n",
            " 2252/10000: episode: 109, duration: 0.551s, episode steps:  62, steps per second: 113, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.052408, mae: 1.375432, mean_q: 2.720924, mean_eps: 0.800155\n",
            " 2284/10000: episode: 110, duration: 0.316s, episode steps:  32, steps per second: 101, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.064992, mae: 1.399071, mean_q: 2.756770, mean_eps: 0.795925\n",
            " 2302/10000: episode: 111, duration: 0.188s, episode steps:  18, steps per second:  96, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.036584, mae: 1.396569, mean_q: 2.796093, mean_eps: 0.793675\n",
            " 2316/10000: episode: 112, duration: 0.146s, episode steps:  14, steps per second:  96, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.065761, mae: 1.422083, mean_q: 2.818949, mean_eps: 0.792235\n",
            " 2355/10000: episode: 113, duration: 0.391s, episode steps:  39, steps per second: 100, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  loss: 0.056494, mae: 1.423013, mean_q: 2.805940, mean_eps: 0.789850\n",
            " 2372/10000: episode: 114, duration: 0.192s, episode steps:  17, steps per second:  89, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.053936, mae: 1.434696, mean_q: 2.825340, mean_eps: 0.787330\n",
            " 2384/10000: episode: 115, duration: 0.114s, episode steps:  12, steps per second: 105, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.065644, mae: 1.456399, mean_q: 2.860992, mean_eps: 0.786025\n",
            " 2410/10000: episode: 116, duration: 0.240s, episode steps:  26, steps per second: 108, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.654 [0.000, 1.000],  loss: 0.069291, mae: 1.453664, mean_q: 2.840886, mean_eps: 0.784315\n",
            " 2428/10000: episode: 117, duration: 0.195s, episode steps:  18, steps per second:  92, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.069990, mae: 1.470616, mean_q: 2.862833, mean_eps: 0.782335\n",
            " 2454/10000: episode: 118, duration: 0.230s, episode steps:  26, steps per second: 113, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.731 [0.000, 1.000],  loss: 0.066846, mae: 1.472160, mean_q: 2.878935, mean_eps: 0.780355\n",
            " 2471/10000: episode: 119, duration: 0.172s, episode steps:  17, steps per second:  99, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.075112, mae: 1.494404, mean_q: 2.930666, mean_eps: 0.778420\n",
            " 2485/10000: episode: 120, duration: 0.141s, episode steps:  14, steps per second: 100, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.062882, mae: 1.483874, mean_q: 2.919837, mean_eps: 0.777025\n",
            " 2502/10000: episode: 121, duration: 0.171s, episode steps:  17, steps per second:  99, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.070984, mae: 1.503563, mean_q: 2.951470, mean_eps: 0.775630\n",
            " 2515/10000: episode: 122, duration: 0.136s, episode steps:  13, steps per second:  95, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.055258, mae: 1.497105, mean_q: 2.947640, mean_eps: 0.774280\n",
            " 2526/10000: episode: 123, duration: 0.115s, episode steps:  11, steps per second:  95, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.056879, mae: 1.516909, mean_q: 2.982627, mean_eps: 0.773200\n",
            " 2540/10000: episode: 124, duration: 0.142s, episode steps:  14, steps per second:  99, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.055219, mae: 1.526459, mean_q: 3.012232, mean_eps: 0.772075\n",
            " 2564/10000: episode: 125, duration: 0.224s, episode steps:  24, steps per second: 107, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.077320, mae: 1.533178, mean_q: 2.995396, mean_eps: 0.770365\n",
            " 2582/10000: episode: 126, duration: 0.166s, episode steps:  18, steps per second: 108, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.069669, mae: 1.535022, mean_q: 3.025489, mean_eps: 0.768475\n",
            " 2596/10000: episode: 127, duration: 0.138s, episode steps:  14, steps per second: 102, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.056757, mae: 1.544975, mean_q: 3.068129, mean_eps: 0.767035\n",
            " 2610/10000: episode: 128, duration: 0.135s, episode steps:  14, steps per second: 104, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.090614, mae: 1.561724, mean_q: 3.035651, mean_eps: 0.765775\n",
            " 2637/10000: episode: 129, duration: 0.265s, episode steps:  27, steps per second: 102, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.072816, mae: 1.557872, mean_q: 3.044331, mean_eps: 0.763930\n",
            " 2656/10000: episode: 130, duration: 0.168s, episode steps:  19, steps per second: 113, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.684 [0.000, 1.000],  loss: 0.084993, mae: 1.582842, mean_q: 3.093283, mean_eps: 0.761860\n",
            " 2679/10000: episode: 131, duration: 0.219s, episode steps:  23, steps per second: 105, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 0.086838, mae: 1.582588, mean_q: 3.088000, mean_eps: 0.759970\n",
            " 2701/10000: episode: 132, duration: 0.213s, episode steps:  22, steps per second: 103, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 0.082293, mae: 1.599945, mean_q: 3.105430, mean_eps: 0.757945\n",
            " 2722/10000: episode: 133, duration: 0.200s, episode steps:  21, steps per second: 105, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.059692, mae: 1.593287, mean_q: 3.139582, mean_eps: 0.756010\n",
            " 2737/10000: episode: 134, duration: 0.156s, episode steps:  15, steps per second:  96, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.053603, mae: 1.613162, mean_q: 3.180860, mean_eps: 0.754390\n",
            " 2759/10000: episode: 135, duration: 0.224s, episode steps:  22, steps per second:  98, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.053542, mae: 1.623120, mean_q: 3.203486, mean_eps: 0.752725\n",
            " 2775/10000: episode: 136, duration: 0.153s, episode steps:  16, steps per second: 105, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.058163, mae: 1.622935, mean_q: 3.200282, mean_eps: 0.751015\n",
            " 2793/10000: episode: 137, duration: 0.173s, episode steps:  18, steps per second: 104, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.111746, mae: 1.649112, mean_q: 3.169837, mean_eps: 0.749485\n",
            " 2802/10000: episode: 138, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.057444, mae: 1.629003, mean_q: 3.189408, mean_eps: 0.748270\n",
            " 2822/10000: episode: 139, duration: 0.200s, episode steps:  20, steps per second: 100, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.091707, mae: 1.647768, mean_q: 3.224667, mean_eps: 0.746965\n",
            " 2839/10000: episode: 140, duration: 0.176s, episode steps:  17, steps per second:  97, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.085288, mae: 1.658863, mean_q: 3.243258, mean_eps: 0.745300\n",
            " 2859/10000: episode: 141, duration: 0.204s, episode steps:  20, steps per second:  98, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.077543, mae: 1.665211, mean_q: 3.225393, mean_eps: 0.743635\n",
            " 2873/10000: episode: 142, duration: 0.135s, episode steps:  14, steps per second: 104, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.101677, mae: 1.689945, mean_q: 3.266356, mean_eps: 0.742105\n",
            " 2883/10000: episode: 143, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.059521, mae: 1.676064, mean_q: 3.291794, mean_eps: 0.741025\n",
            " 2907/10000: episode: 144, duration: 0.227s, episode steps:  24, steps per second: 106, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.082635, mae: 1.694461, mean_q: 3.300782, mean_eps: 0.739495\n",
            " 2919/10000: episode: 145, duration: 0.115s, episode steps:  12, steps per second: 104, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.092158, mae: 1.690373, mean_q: 3.320770, mean_eps: 0.737875\n",
            " 2933/10000: episode: 146, duration: 0.138s, episode steps:  14, steps per second: 101, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.077719, mae: 1.700881, mean_q: 3.318070, mean_eps: 0.736705\n",
            " 2947/10000: episode: 147, duration: 0.137s, episode steps:  14, steps per second: 102, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.088915, mae: 1.709377, mean_q: 3.329945, mean_eps: 0.735445\n",
            " 2973/10000: episode: 148, duration: 0.257s, episode steps:  26, steps per second: 101, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.097377, mae: 1.715113, mean_q: 3.310930, mean_eps: 0.733645\n",
            " 3012/10000: episode: 149, duration: 0.357s, episode steps:  39, steps per second: 109, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.590 [0.000, 1.000],  loss: 0.082297, mae: 1.725654, mean_q: 3.363723, mean_eps: 0.730720\n",
            " 3022/10000: episode: 150, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.102157, mae: 1.748292, mean_q: 3.395347, mean_eps: 0.728515\n",
            " 3037/10000: episode: 151, duration: 0.146s, episode steps:  15, steps per second: 103, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.081487, mae: 1.736861, mean_q: 3.420480, mean_eps: 0.727390\n",
            " 3046/10000: episode: 152, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.090745, mae: 1.745470, mean_q: 3.398906, mean_eps: 0.726310\n",
            " 3069/10000: episode: 153, duration: 0.215s, episode steps:  23, steps per second: 107, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.652 [0.000, 1.000],  loss: 0.089744, mae: 1.764182, mean_q: 3.444538, mean_eps: 0.724870\n",
            " 3085/10000: episode: 154, duration: 0.154s, episode steps:  16, steps per second: 104, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.101167, mae: 1.768124, mean_q: 3.453634, mean_eps: 0.723115\n",
            " 3111/10000: episode: 155, duration: 0.233s, episode steps:  26, steps per second: 112, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.092930, mae: 1.781981, mean_q: 3.460866, mean_eps: 0.721225\n",
            " 3135/10000: episode: 156, duration: 0.218s, episode steps:  24, steps per second: 110, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.102097, mae: 1.797782, mean_q: 3.445329, mean_eps: 0.718975\n",
            " 3194/10000: episode: 157, duration: 0.515s, episode steps:  59, steps per second: 115, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 0.089324, mae: 1.807386, mean_q: 3.522948, mean_eps: 0.715240\n",
            " 3211/10000: episode: 158, duration: 0.148s, episode steps:  17, steps per second: 115, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.077356, mae: 1.831941, mean_q: 3.556402, mean_eps: 0.711820\n",
            " 3227/10000: episode: 159, duration: 0.150s, episode steps:  16, steps per second: 107, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.086263, mae: 1.827629, mean_q: 3.543158, mean_eps: 0.710335\n",
            " 3237/10000: episode: 160, duration: 0.092s, episode steps:  10, steps per second: 109, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.087136, mae: 1.837498, mean_q: 3.593828, mean_eps: 0.709165\n",
            " 3251/10000: episode: 161, duration: 0.130s, episode steps:  14, steps per second: 107, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.117738, mae: 1.848372, mean_q: 3.590468, mean_eps: 0.708085\n",
            " 3317/10000: episode: 162, duration: 0.582s, episode steps:  66, steps per second: 113, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.576 [0.000, 1.000],  loss: 0.095878, mae: 1.856758, mean_q: 3.587467, mean_eps: 0.704485\n",
            " 3332/10000: episode: 163, duration: 0.135s, episode steps:  15, steps per second: 111, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.106258, mae: 1.866032, mean_q: 3.593314, mean_eps: 0.700840\n",
            " 3351/10000: episode: 164, duration: 0.178s, episode steps:  19, steps per second: 107, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.105587, mae: 1.877177, mean_q: 3.661718, mean_eps: 0.699310\n",
            " 3370/10000: episode: 165, duration: 0.170s, episode steps:  19, steps per second: 112, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.737 [0.000, 1.000],  loss: 0.094929, mae: 1.890590, mean_q: 3.678774, mean_eps: 0.697600\n",
            " 3385/10000: episode: 166, duration: 0.145s, episode steps:  15, steps per second: 103, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.058260, mae: 1.893295, mean_q: 3.726239, mean_eps: 0.696070\n",
            " 3398/10000: episode: 167, duration: 0.116s, episode steps:  13, steps per second: 112, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.114991, mae: 1.912587, mean_q: 3.706346, mean_eps: 0.694810\n",
            " 3413/10000: episode: 168, duration: 0.143s, episode steps:  15, steps per second: 105, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.117657, mae: 1.907693, mean_q: 3.673167, mean_eps: 0.693550\n",
            " 3425/10000: episode: 169, duration: 0.115s, episode steps:  12, steps per second: 105, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.119524, mae: 1.910539, mean_q: 3.702418, mean_eps: 0.692335\n",
            " 3472/10000: episode: 170, duration: 0.417s, episode steps:  47, steps per second: 113, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 0.089339, mae: 1.926052, mean_q: 3.771639, mean_eps: 0.689680\n",
            " 3484/10000: episode: 171, duration: 0.116s, episode steps:  12, steps per second: 103, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.095144, mae: 1.931415, mean_q: 3.781754, mean_eps: 0.687025\n",
            " 3525/10000: episode: 172, duration: 0.359s, episode steps:  41, steps per second: 114, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 0.083065, mae: 1.940289, mean_q: 3.787214, mean_eps: 0.684640\n",
            " 3545/10000: episode: 173, duration: 0.177s, episode steps:  20, steps per second: 113, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.133028, mae: 1.964453, mean_q: 3.795326, mean_eps: 0.681895\n",
            " 3569/10000: episode: 174, duration: 0.223s, episode steps:  24, steps per second: 108, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.121454, mae: 1.966948, mean_q: 3.758316, mean_eps: 0.679915\n",
            " 3585/10000: episode: 175, duration: 0.149s, episode steps:  16, steps per second: 108, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.132421, mae: 1.998255, mean_q: 3.861541, mean_eps: 0.678115\n",
            " 3602/10000: episode: 176, duration: 0.152s, episode steps:  17, steps per second: 112, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.066762, mae: 1.977966, mean_q: 3.866700, mean_eps: 0.676630\n",
            " 3615/10000: episode: 177, duration: 0.126s, episode steps:  13, steps per second: 103, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.095621, mae: 2.001237, mean_q: 3.895648, mean_eps: 0.675280\n",
            " 3645/10000: episode: 178, duration: 0.269s, episode steps:  30, steps per second: 111, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  loss: 0.101352, mae: 2.002896, mean_q: 3.880656, mean_eps: 0.673345\n",
            " 3655/10000: episode: 179, duration: 0.094s, episode steps:  10, steps per second: 106, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.073574, mae: 2.012995, mean_q: 3.951696, mean_eps: 0.671545\n",
            " 3667/10000: episode: 180, duration: 0.116s, episode steps:  12, steps per second: 103, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.128009, mae: 2.020039, mean_q: 3.923416, mean_eps: 0.670555\n",
            " 3676/10000: episode: 181, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.122085, mae: 2.003562, mean_q: 3.852512, mean_eps: 0.669610\n",
            " 3689/10000: episode: 182, duration: 0.129s, episode steps:  13, steps per second: 101, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.088113, mae: 2.011205, mean_q: 3.935290, mean_eps: 0.668620\n",
            " 3705/10000: episode: 183, duration: 0.138s, episode steps:  16, steps per second: 116, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.114480, mae: 2.032105, mean_q: 3.939900, mean_eps: 0.667315\n",
            " 3729/10000: episode: 184, duration: 0.224s, episode steps:  24, steps per second: 107, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.109050, mae: 2.029004, mean_q: 3.917917, mean_eps: 0.665515\n",
            " 3758/10000: episode: 185, duration: 0.256s, episode steps:  29, steps per second: 113, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 0.098874, mae: 2.051959, mean_q: 3.958074, mean_eps: 0.663130\n",
            " 3786/10000: episode: 186, duration: 0.252s, episode steps:  28, steps per second: 111, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.107870, mae: 2.069581, mean_q: 4.004676, mean_eps: 0.660565\n",
            " 3802/10000: episode: 187, duration: 0.145s, episode steps:  16, steps per second: 110, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.147570, mae: 2.097634, mean_q: 4.036301, mean_eps: 0.658585\n",
            " 3831/10000: episode: 188, duration: 0.266s, episode steps:  29, steps per second: 109, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  loss: 0.093687, mae: 2.082855, mean_q: 4.046649, mean_eps: 0.656560\n",
            " 3849/10000: episode: 189, duration: 0.154s, episode steps:  18, steps per second: 117, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.115430, mae: 2.081954, mean_q: 4.031003, mean_eps: 0.654445\n",
            " 3861/10000: episode: 190, duration: 0.112s, episode steps:  12, steps per second: 107, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.141730, mae: 2.093011, mean_q: 4.018496, mean_eps: 0.653095\n",
            " 3896/10000: episode: 191, duration: 0.313s, episode steps:  35, steps per second: 112, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 0.119835, mae: 2.100826, mean_q: 4.035894, mean_eps: 0.650980\n",
            " 3914/10000: episode: 192, duration: 0.164s, episode steps:  18, steps per second: 110, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.145085, mae: 2.124427, mean_q: 4.078912, mean_eps: 0.648595\n",
            " 3929/10000: episode: 193, duration: 0.138s, episode steps:  15, steps per second: 109, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.117618, mae: 2.113604, mean_q: 4.086948, mean_eps: 0.647110\n",
            " 3945/10000: episode: 194, duration: 0.149s, episode steps:  16, steps per second: 107, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.112941, mae: 2.107611, mean_q: 4.081348, mean_eps: 0.645715\n",
            " 3959/10000: episode: 195, duration: 0.128s, episode steps:  14, steps per second: 110, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.130589, mae: 2.144373, mean_q: 4.132853, mean_eps: 0.644365\n",
            " 3980/10000: episode: 196, duration: 0.192s, episode steps:  21, steps per second: 110, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.124357, mae: 2.134737, mean_q: 4.100585, mean_eps: 0.642790\n",
            " 4001/10000: episode: 197, duration: 0.190s, episode steps:  21, steps per second: 111, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.143315, mae: 2.156068, mean_q: 4.147366, mean_eps: 0.640900\n",
            " 4033/10000: episode: 198, duration: 0.281s, episode steps:  32, steps per second: 114, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.127801, mae: 2.153538, mean_q: 4.140658, mean_eps: 0.638515\n",
            " 4056/10000: episode: 199, duration: 0.217s, episode steps:  23, steps per second: 106, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.120607, mae: 2.173861, mean_q: 4.218305, mean_eps: 0.636040\n",
            " 4086/10000: episode: 200, duration: 0.289s, episode steps:  30, steps per second: 104, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 0.117742, mae: 2.178954, mean_q: 4.225150, mean_eps: 0.633655\n",
            " 4125/10000: episode: 201, duration: 0.359s, episode steps:  39, steps per second: 109, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 0.108184, mae: 2.187550, mean_q: 4.257476, mean_eps: 0.630550\n",
            " 4154/10000: episode: 202, duration: 0.275s, episode steps:  29, steps per second: 106, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.119795, mae: 2.211317, mean_q: 4.274296, mean_eps: 0.627490\n",
            " 4179/10000: episode: 203, duration: 0.247s, episode steps:  25, steps per second: 101, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.104150, mae: 2.224896, mean_q: 4.321854, mean_eps: 0.625060\n",
            " 4202/10000: episode: 204, duration: 0.225s, episode steps:  23, steps per second: 102, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.348 [0.000, 1.000],  loss: 0.118766, mae: 2.234176, mean_q: 4.331391, mean_eps: 0.622900\n",
            " 4250/10000: episode: 205, duration: 0.419s, episode steps:  48, steps per second: 114, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.135853, mae: 2.249472, mean_q: 4.346034, mean_eps: 0.619705\n",
            " 4290/10000: episode: 206, duration: 0.361s, episode steps:  40, steps per second: 111, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.109524, mae: 2.258618, mean_q: 4.393106, mean_eps: 0.615745\n",
            " 4423/10000: episode: 207, duration: 1.149s, episode steps: 133, steps per second: 116, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 0.123929, mae: 2.296962, mean_q: 4.455310, mean_eps: 0.607960\n",
            " 4458/10000: episode: 208, duration: 0.306s, episode steps:  35, steps per second: 114, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 0.119235, mae: 2.309858, mean_q: 4.477209, mean_eps: 0.600400\n",
            " 4496/10000: episode: 209, duration: 0.340s, episode steps:  38, steps per second: 112, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.116063, mae: 2.334892, mean_q: 4.528825, mean_eps: 0.597115\n",
            " 4537/10000: episode: 210, duration: 0.383s, episode steps:  41, steps per second: 107, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.122816, mae: 2.364282, mean_q: 4.608827, mean_eps: 0.593560\n",
            " 4559/10000: episode: 211, duration: 0.206s, episode steps:  22, steps per second: 107, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.128529, mae: 2.347704, mean_q: 4.538839, mean_eps: 0.590725\n",
            " 4582/10000: episode: 212, duration: 0.204s, episode steps:  23, steps per second: 113, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.123809, mae: 2.367868, mean_q: 4.594098, mean_eps: 0.588700\n",
            " 4674/10000: episode: 213, duration: 0.816s, episode steps:  92, steps per second: 113, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.119329, mae: 2.404188, mean_q: 4.693760, mean_eps: 0.583525\n",
            " 4721/10000: episode: 214, duration: 0.404s, episode steps:  47, steps per second: 116, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 0.133505, mae: 2.434483, mean_q: 4.733650, mean_eps: 0.577270\n",
            " 4767/10000: episode: 215, duration: 0.411s, episode steps:  46, steps per second: 112, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.134476, mae: 2.452389, mean_q: 4.774867, mean_eps: 0.573085\n",
            " 4802/10000: episode: 216, duration: 0.304s, episode steps:  35, steps per second: 115, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.126835, mae: 2.461485, mean_q: 4.816361, mean_eps: 0.569440\n",
            " 4858/10000: episode: 217, duration: 0.499s, episode steps:  56, steps per second: 112, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 0.141479, mae: 2.478474, mean_q: 4.844556, mean_eps: 0.565345\n",
            " 4886/10000: episode: 218, duration: 0.253s, episode steps:  28, steps per second: 111, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.118865, mae: 2.498398, mean_q: 4.892497, mean_eps: 0.561565\n",
            " 4940/10000: episode: 219, duration: 0.462s, episode steps:  54, steps per second: 117, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.114356, mae: 2.520493, mean_q: 4.946290, mean_eps: 0.557875\n",
            " 5018/10000: episode: 220, duration: 0.687s, episode steps:  78, steps per second: 113, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.410 [0.000, 1.000],  loss: 0.135327, mae: 2.543289, mean_q: 4.962578, mean_eps: 0.551935\n",
            " 5077/10000: episode: 221, duration: 0.506s, episode steps:  59, steps per second: 116, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.373 [0.000, 1.000],  loss: 0.147606, mae: 2.570899, mean_q: 5.019281, mean_eps: 0.545770\n",
            " 5137/10000: episode: 222, duration: 0.534s, episode steps:  60, steps per second: 112, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.114713, mae: 2.595763, mean_q: 5.096281, mean_eps: 0.540415\n",
            " 5200/10000: episode: 223, duration: 0.555s, episode steps:  63, steps per second: 114, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.365 [0.000, 1.000],  loss: 0.126027, mae: 2.622616, mean_q: 5.156416, mean_eps: 0.534880\n",
            " 5235/10000: episode: 224, duration: 0.331s, episode steps:  35, steps per second: 106, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.131259, mae: 2.646740, mean_q: 5.182159, mean_eps: 0.530470\n",
            " 5297/10000: episode: 225, duration: 0.531s, episode steps:  62, steps per second: 117, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.197986, mae: 2.677215, mean_q: 5.184852, mean_eps: 0.526105\n",
            " 5497/10000: episode: 226, duration: 1.739s, episode steps: 200, steps per second: 115, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.157959, mae: 2.716743, mean_q: 5.321338, mean_eps: 0.514315\n",
            " 5611/10000: episode: 227, duration: 0.985s, episode steps: 114, steps per second: 116, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 0.125359, mae: 2.770454, mean_q: 5.464576, mean_eps: 0.500185\n",
            " 5702/10000: episode: 228, duration: 0.791s, episode steps:  91, steps per second: 115, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 0.140417, mae: 2.826537, mean_q: 5.567679, mean_eps: 0.490960\n",
            " 5785/10000: episode: 229, duration: 0.707s, episode steps:  83, steps per second: 117, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.434 [0.000, 1.000],  loss: 0.153368, mae: 2.856721, mean_q: 5.635916, mean_eps: 0.483130\n",
            " 5903/10000: episode: 230, duration: 1.015s, episode steps: 118, steps per second: 116, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 0.142344, mae: 2.896782, mean_q: 5.692856, mean_eps: 0.474085\n",
            " 5978/10000: episode: 231, duration: 0.645s, episode steps:  75, steps per second: 116, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.157575, mae: 2.942346, mean_q: 5.787262, mean_eps: 0.465400\n",
            " 6061/10000: episode: 232, duration: 0.740s, episode steps:  83, steps per second: 112, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 0.148548, mae: 2.982427, mean_q: 5.884672, mean_eps: 0.458290\n",
            " 6075/10000: episode: 233, duration: 0.123s, episode steps:  14, steps per second: 113, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.156189, mae: 3.002829, mean_q: 5.955954, mean_eps: 0.453925\n",
            " 6151/10000: episode: 234, duration: 0.669s, episode steps:  76, steps per second: 114, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 0.166320, mae: 3.022802, mean_q: 5.970857, mean_eps: 0.449875\n",
            " 6276/10000: episode: 235, duration: 1.086s, episode steps: 125, steps per second: 115, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 0.178944, mae: 3.058224, mean_q: 6.028886, mean_eps: 0.440830\n",
            " 6397/10000: episode: 236, duration: 1.061s, episode steps: 121, steps per second: 114, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 0.145848, mae: 3.110269, mean_q: 6.153336, mean_eps: 0.429760\n",
            " 6500/10000: episode: 237, duration: 0.881s, episode steps: 103, steps per second: 117, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 0.155180, mae: 3.159438, mean_q: 6.267156, mean_eps: 0.419680\n",
            " 6601/10000: episode: 238, duration: 0.856s, episode steps: 101, steps per second: 118, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.174486, mae: 3.205075, mean_q: 6.331406, mean_eps: 0.410500\n",
            " 6712/10000: episode: 239, duration: 0.945s, episode steps: 111, steps per second: 118, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.182213, mae: 3.241095, mean_q: 6.401123, mean_eps: 0.400960\n",
            " 6833/10000: episode: 240, duration: 1.046s, episode steps: 121, steps per second: 116, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 0.177956, mae: 3.308666, mean_q: 6.563820, mean_eps: 0.390520\n",
            " 7033/10000: episode: 241, duration: 1.694s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.174344, mae: 3.364303, mean_q: 6.665975, mean_eps: 0.376075\n",
            " 7165/10000: episode: 242, duration: 1.127s, episode steps: 132, steps per second: 117, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 0.199380, mae: 3.435954, mean_q: 6.802818, mean_eps: 0.361135\n",
            " 7328/10000: episode: 243, duration: 1.438s, episode steps: 163, steps per second: 113, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 0.194465, mae: 3.506166, mean_q: 6.959867, mean_eps: 0.347860\n",
            " 7460/10000: episode: 244, duration: 1.133s, episode steps: 132, steps per second: 117, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 0.157021, mae: 3.568334, mean_q: 7.117733, mean_eps: 0.334585\n",
            " 7584/10000: episode: 245, duration: 1.106s, episode steps: 124, steps per second: 112, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 0.203491, mae: 3.625239, mean_q: 7.205102, mean_eps: 0.323065\n",
            " 7705/10000: episode: 246, duration: 1.058s, episode steps: 121, steps per second: 114, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 0.188124, mae: 3.674816, mean_q: 7.295467, mean_eps: 0.312040\n",
            " 7905/10000: episode: 247, duration: 1.770s, episode steps: 200, steps per second: 113, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.210201, mae: 3.752541, mean_q: 7.444778, mean_eps: 0.297595\n",
            " 8105/10000: episode: 248, duration: 1.708s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.195541, mae: 3.831889, mean_q: 7.632102, mean_eps: 0.279595\n",
            " 8257/10000: episode: 249, duration: 1.322s, episode steps: 152, steps per second: 115, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 0.246762, mae: 3.896758, mean_q: 7.757918, mean_eps: 0.263755\n",
            " 8457/10000: episode: 250, duration: 1.740s, episode steps: 200, steps per second: 115, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.225466, mae: 3.984994, mean_q: 7.944035, mean_eps: 0.247915\n",
            " 8657/10000: episode: 251, duration: 1.711s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 0.202383, mae: 4.079155, mean_q: 8.170275, mean_eps: 0.229915\n",
            " 8773/10000: episode: 252, duration: 1.072s, episode steps: 116, steps per second: 108, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.243974, mae: 4.157624, mean_q: 8.314796, mean_eps: 0.215695\n",
            " 8973/10000: episode: 253, duration: 1.695s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.201035, mae: 4.218125, mean_q: 8.455531, mean_eps: 0.201475\n",
            " 9102/10000: episode: 254, duration: 1.105s, episode steps: 129, steps per second: 117, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 0.266264, mae: 4.286614, mean_q: 8.546068, mean_eps: 0.186670\n",
            " 9262/10000: episode: 255, duration: 1.421s, episode steps: 160, steps per second: 113, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 0.248588, mae: 4.356674, mean_q: 8.681970, mean_eps: 0.173665\n",
            " 9388/10000: episode: 256, duration: 1.082s, episode steps: 126, steps per second: 116, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 0.248395, mae: 4.423629, mean_q: 8.818790, mean_eps: 0.160795\n",
            " 9535/10000: episode: 257, duration: 1.248s, episode steps: 147, steps per second: 118, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.320342, mae: 4.471070, mean_q: 8.912254, mean_eps: 0.148510\n",
            " 9702/10000: episode: 258, duration: 1.486s, episode steps: 167, steps per second: 112, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 0.277521, mae: 4.552618, mean_q: 9.078706, mean_eps: 0.134380\n",
            " 9827/10000: episode: 259, duration: 1.107s, episode steps: 125, steps per second: 113, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.361856, mae: 4.615897, mean_q: 9.178218, mean_eps: 0.121240\n",
            "done, took 99.963 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 149.000, steps: 149\n",
            "Episode 2: reward: 173.000, steps: 173\n",
            "Episode 3: reward: 135.000, steps: 135\n",
            "Episode 4: reward: 129.000, steps: 129\n",
            "Episode 5: reward: 137.000, steps: 137\n",
            "Episode 6: reward: 173.000, steps: 173\n",
            "Episode 7: reward: 143.000, steps: 143\n",
            "Episode 8: reward: 147.000, steps: 147\n",
            "Episode 9: reward: 137.000, steps: 137\n",
            "Episode 10: reward: 121.000, steps: 121\n",
            "Episode 11: reward: 125.000, steps: 125\n",
            "Episode 12: reward: 161.000, steps: 161\n",
            "Episode 13: reward: 161.000, steps: 161\n",
            "Episode 14: reward: 131.000, steps: 131\n",
            "Episode 15: reward: 166.000, steps: 166\n",
            "Episode 16: reward: 142.000, steps: 142\n",
            "Episode 17: reward: 185.000, steps: 185\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 190.000, steps: 190\n",
            "Episode 20: reward: 143.000, steps: 143\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7feec2f78410>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jowcvc8-t8Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Gls5LXIVUil"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}